# Rating Scale Model {#RS_model}

## Rasch Rating Scale Model 

The Rasch Rating Scale Model (RSM; sometimes also called the Polytomous Rasch model) was developed by [Andrich(1978)](https://link.springer.com/article/10.1007/BF02293814) for polytomous data (data with >= 2 ordinal categories). It provides estimates of a; *Person locations*, b; *Item Difficulties* and c; *An overall set of thresholds (fixed across items)*.

Rating Scale Model Equation

$$ln\left[\frac{P_{n_i(x=k)}}{P_{n_i(x=k-1)}}\right]=\theta_{n}-\delta_{i}-\tau_{k}$$
Where $\theta$ is the person's ability, $\delta$ is the item's difficulty, and $\tau$ is the thresholds which are estimated empirically for the whole set of items. The RSM assumes that the **threshold structure** is fixed across items. The relative distance between thresholds is the same across items, but items still have different difficulties. The thresholds just move up or down the logit scale.
 
## R-Lab: Rasch Rating Scale Model with "eRm" package

### Load the packages that required for the Rasch Rating Scale Analysis
```{r message=FALSE, warning=FALSE}
library(readr) # For import the data
library(TAM) # For running the Rating Scale Rasch Model
library(plyr) # For plot the Item characteristic curves
library(WrightMap)# For plot the variable map
library(eRm) # For another example
```

### Information about the data
We are going to practice running the Rating Scale (RS) model using the "TAM" package. Specifically, we will be working with data from a writing assessment in which students received polytomous ratings on essays. Several researchers have used this dataset in published studies, and it is considered a classic example of rating data. 

The original data collection design is described by [Braun(1988)](https://onlinelibrary.wiley.com/doi/epdf/10.1002/j.2330-8516.1988.tb00281.x). The original dataset includes ratings for 32 students by 12 raters on three separate essay compositions. For this lab, we will look at the data from Essay 1. For ease of interpretation, the essay ratings from the original dataset have been recoded from nine categories to three categories (1 = low achievement, 2 = middle achievement; 3 = high achievement).

In our analysis, we will treat the 12 raters as 12 polytomous “items” that share the three-category rating scale structure. Raters with high “difficulty” calibrations can be interpreted as severe – these raters assign low scores more often. Raters with low “difficulty” calibrations can be interpreted as lenient – these raters assign high scores more often.

### Get Data Prepared
we will try running a polytomous Rasch model using some of the example data that is provided with the TAM package.

***Note** If you want to use the "eRm" package to run the Rating Scale model, you need to make sure that each item has responses in the same categories as each of the other items. 

```{r message=FALSE}
# Load the data
braun_data <- read.csv("braun data.csv")
head(braun_data)
# Get a quick overview of the data with the summary() function
summary(braun_data)
# Trim the student id variable from the dataframe because the TAM package only needs response matrix.
rs_data <- braun_data[,-1]
```

### Run the Rating Scale Model 
```{r message=FALSE, results='hide'}
# Run the Rating Scale Model
rs_model <- TAM::tam.mml(rs_data, irtmodel="RSM") 
```

```{r}
# Check the model summary
summary(rs_model)
```

### Wright Map & Expected Response Curves & Item characteristic curves 
Wright Map or Variable Map
```{r}
# Plot the Variable Map
graphics.off() # In case you can not run the plot correctly
IRT.WrightMap(rs_model,show.thr.lab=TRUE)  
```

Expected Response Curves
```{r}
# Plot expected response curves 
plot(rs_model,ask=FALSE)
```

Item characteristic curves (but now as thresholds)
```{r}
graphics.off()
plot(rs_model, type="items")
```

### Item estimates and fit Statistics
```{r}
# We can use the similar code to achieve the item estimate as what we did for the Dichotomous Analysis
graphics.off()
rs_model$xsi # The first column is the item difficulty. In this case, is the rater's rating severity.
rater_estimates <- rs_model$xsi
tam.fit(rs_model) 
# Note the last two rows also provides you the average fit statistics for category 1 and category 2. For this analysis, we are not focus on these data.
# We can also check the Rating Scale Thresholds
rs_threshold <- tam.threshold(rs_model)
rs_threshold # This provides the detail logit location for each categories for each rater.
```
**Note** The tam.threshold() function is actually calculating Thurstonian thresholds, whereas the tau estimates are [Andrich thresholds](https://www.winsteps.com/winman/disorder.htm). These are different parameters. 

The Thurstonian thresholds are cumulative, meaning that they reflect the probability for responding in a category of interest or any higher category. The Andrich thresholds are adjacent-categories thresholds, which reflect the point on the logit scale at which there is an equal probability for a rating in a category of interest or the category just below it. You can check [here](https://www.winsteps.com/winman/ratingscale.htm) for more information.

### Person estimates and fit Statistics
```{r}
# Use the tam.wle function to acheive the person ability
person_ability <- tam.wle(rs_model)
# Print out the person ability
head(person_ability$theta)# Person's fit statistics
rs_personfit <- tam.personfit(rs_model)
# Check the first 6 students' person fit statistics
head(rs_personfit)
```

## Another Example with "eRm" package

The "eRm" package also provide the function to run a polytomous Rasch model. In this example, we will use a build-in data set in "eRm" package as an example.

### Load the example data

```{r}
### Load the example data:
data("rsmdat")
# These data include 20 participants’ responses to six items that included four ordered categories (0, 1, 2, and 3).
summary(rsmdat)
```

**Important!**
Note that in the summary of the "rsmdat" object, there are responses in all 4 categories for all items. This means that it is possible to run the Rating Scale model on these item responses. If participants did not use all of the categories on all of the items, then we would need to run the Partial Credit model instead. 

### Run the Rating Scale model on the data 

```{r}
rs_model <- RSM(rsmdat)
```


### Examine item difficulty and threshold SE


```{r}
### Examine item difficulty values:
item.estimates <- thresholds(rs_model)
item.estimates
## Get threshold SEs values:
item.se <- item.estimates$se.thresh
item.se
```

### Examine Person locations (theta) and SEs
```{r}
# Standard errors for theta estimates:
person.locations.estimate <- person.parameter(rs_model)
summary(person.locations.estimate)
```

### Exam the item and person fit statistics
```{r}
item.fit <- itemfit(person.locations.estimate)
item.fit
pfit <- personfit(person.locations.estimate)
pfit
```

**Note:** This procedure will give us information about the infit and outfit statistics for each item. Please review our lecture materials for details about the interpretation of these values, noting that we generally expect these statistics to be around 1.00.

### Graphic Information

#### Plot the Person-Item Map
```{r}
plotPImap(rs_model, sorted = TRUE)
```

In this plot, we should consider the degree to which there is evidence of overlap between item and person locations (targeting).

We can also examine the individual items’ ordering on the logit scale with reference to our theory about the expected ordering.

#### Plot the Standardized Residuals
```{r}
stresid <- item.fit$st.res

# before constructing the plots, find the max & min residuals:
max.resid <- ceiling(max(stresid))
min.resid <- ceiling(min(stresid))

for(item.number in 1:ncol(stresid)){
  plot(stresid[, item.number], ylim = c(min.resid, max.resid),
       main = paste("Standardized Residuals for Item ", item.number, sep = ""),
       ylab = "Standardized Residual", xlab = "Person Index")
  abline(h = 0, col = "blue")
  abline(h=2, lty = 2, col = "red")
  abline(h=-2, lty = 2, col = "red")
  legend("topright", c("Std. Residual", "Observed = Expected", "+/- 2 SD"), pch = c(1, NA, NA), 
         lty = c(NA, 1, 2),
         col = c("black", "blue", "red"), cex = .8)
}

```

#### Plot the empirical IRFs
```{r}
for(item.number in 1:ncol(stresid)){
  plotICC(rs_model, item.subset = item.number, empICC = list("raw"), empCI = list())
}
```



## Supplmentary Learning Materials 
Andrich, D(1978). “A rating formulation for ordered response categories.” Psychometrika,
43(4), 561–573. doi:10.1007/BF02293814.

Mair, P., Hatzinger, R., & Maier M. J. (2020). eRm: Extended Rasch Modeling. 1.0-1.   https://cran.r-project.org/package=eRm


