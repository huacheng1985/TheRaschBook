# Dichotomous Rasch Model {#Dich_Rasch_model}

This chapter provides a basic overview of the dichotomous Rasch model, along with guidance for analyzing data using the dichotomous Rasch model with R. At the end of the chapter, a challenge exercise and resources for further study are presented.

## Dichotomous Rasch Model

The dichotomous Rasch model (Rasch, 1960) is the simplest model in the Rasch family of models (Wright & Mok, 2004). It was designed for use with ordinal data that are scored in two categories (usually 0 or 1). The dichotomous Rasch model uses sum scores from these ordinal responses to calculate interval-level estimates that represent person locations (i.e., person ability or person achievement) and item locations (i.e., the difficulty to provide a correct or positive response) on a linear scale that represents the latent variable (the log-odds or "logit" scale). The difference between person and item locations can be used to calculate the probability for a correct or positive response (x = 1), rather than an incorrect or negative response (x = 0).

### Model Equation

The equation for the dichotomous Rasch model can be expressed in log-odds form as follows:
  
  $$\ln_{}{}\left[\frac{\phi_{n i 1}}{\phi_{n i 0}}\right]=\theta_{n}-\delta_{i}$$
  
  The Rasch model predicts the probability of person *n* on item *i* providing a correct or positive (x = 1), rather than an incorrect or negative (x = 0) response, given a person’s “ability” (*θn*) and an item’s difficulty (*δi*), as expressed on the logit scale.

### Model Requirements

Estimates that are calculated using the dichotomous Rasch model can only be meaningfully interpreted if there is evidence that the data approximate the requirements for the model. Key among dichotomous Rasch model requirements are the following:
  
  * Unidimensionality: A single latent variable is sufficient to explain most of the variation in item responses
* Local independence: After controlling for the latent variable, there is no meaningful relationship between the responses to individual items
* Person-invariant item estimates: Item locations do not depend on (i.e., are independent from) the persons whose responses are used to estimmate them
* Item-invariant person estimates: Person locations  do not depend on (i.e., are independent from) the items used to estimate them

Evidence that data approximate these requirements provides support for the meaningful interpretation and use of item and person estimates on the logit scale as indicators of item and person locations on the latent variable. In practice, many analysts evaluate these requirements using indicators of model-data fit for the facets in a Rasch model (in this case, items and persons). In the current chapter, we provide some basic code for calculating fit indices for items and persons. We explore this topic in more detail in Chapter 3.


## Running Dichotomous Rasch Model in R
### Understand the Data set
In this example,we will be working with data from a transitive reasoning test, which is a reasoning test related to relationships among physical objects. The transitive reasoning data were collected from a one-on-one interactive assessment in which an experimenter presented students with a set of objects, such as sticks, balls, cubes, and discs. The following description is given in [Sijtsma and Molenaar](https://methods.sagepub.com/book/introduction-to-nonparametric-item-response-theory) (2002), pp. 31-32:
  
  > The items for transitive reasoning had the following structure. A typical item used three sticks, here denoted A, B, and C, of different length, denoted Y, such that YA < YB < YC. The actual test taking had the form of a conversation between experimenter and child in which the sticks were identified by their colors rather than letters. First, sticks A and B were presented to a child, who was allowed to pick them up and compare their lengths, for example, by placing them next to each other on a table. 

> Next, sticks B and C were presented and compared. Then all three sticks were displayed in a random order at large mutual distances so that their length differences were imperceptible, and the child was asked to infer the relation between sticks A and C from his or her knowledge of the relationship in the other two pairs.

The transitive reasoning items varied in terms of the property students were asked to reason about (length, weight, area). The tasks also varied in terms of the number of items students were asked to reason about, and whether the tasks involved equalities, inequalities, or a mixture of equalities and inequalities. The characteristics of the transitive reasoning data are summarized in the following table:
  
  |Task|Property|Format|Objects|Measures|
  | :--- | :---- |  :---- | :---- | :---- |
  |1|Length|YA > YB > YC|Sticks|12, 11.5, 11 (cm)|
  |2|Length|YA = YB = YC = YD |Tubes|12 (cm)|
  |3|Weight|YA > YB > YC|Tubes|45, 25, 18 (g)|
  |4|Weight|YA = YB = YC = YD|Cubes|65 (g)|
  |5|Weight|YA < YB < YC|Balls|40, 50, 70 (g)|
  |6|Area|YA > YB> YC|Discs|2.5, 7, 6.5 (diameter; cm)|
  |7|Length|YA > YB = YC|Sticks|28.5, 27.5, 27.5 (cm)|
  |8|Weight|YA >YB = YC|Balls|65, 40, 40 (g)|
  |9|Length|YA = YB = YC = YD|Sticks|12.5, 12.5, 13, 13 (cm)|
  |10|Weight|YA = YB < YC = YD|Balls|60, 60, 100, 100 (g)|
  
  
### The TAM Package

We will use the *"Test Analysis Modules"*, or "TAM" package (Robitzsch, Kiefer, & Wu, 2020) to run the dichotomous Rasch model analyses in this chapter. 

The TAM package applies marginal maximum likelihood estimation (MMLE) to estimate the dichotomous Rasch model, and the package includes several useful functions related to the dichotomous Rasch model.

To get started with the TAM package, install and load it using the following code:
```{r}
install.packages("TAM")
library("TAM")
```

To facilitate the example analysis, we will also use three other packages:
  
  WrightMap: XXXX
```{r}
install.packages("WrightMap")
library("WrightMap") 
```

Hmisc: XXXX

```{r}
install.packages("Hmisc")
library("Hmisc") 
```

formattable: XXXX

```{r}
install.packages("formattable")
library("formattable") 
```


### Getting Started with the Analysis

Now that we have installed and loaded the packages to our R session, we are ready to import the data. 

In this book, we use the function read.csv() to import data that are stored using comma separated values. We encourage readers to use their preferred method for importing data files into R or R Studio.

Please note that if you use read.csv() you will need to specify the file path to the location at which the data file is stored on your computer or set your working directory to the folder in which you have saved the data.

First, we will import the data using read.csv() and store it in an object called "transreas":

```{r}
transreas <- read.csv("transreas.csv")
```

Next, we will explore the data using descriptive statistics using the summary() function:

```{r}
# Use the summary() function to calculate descriptive statistics for each variable in the dataset
summary(transreas)
```
From the result, we can see there are no missing data. We can also get a general sense of the scales, range, and distribution of each variable in the dataset. For example, the mean for each task is the proportion correct statistic (item difficulty estimate for Classical Test Theory).


### Run the Dichotomous Rasch Model

To run the dichotomous Rasch Model using the TAM package, need to isolate the item responses from the other variables in the data (Student IDs and Grade level). 

We will create an object made up of only the item responses by removing the first two variables from the data, and then check the data to ensure it is ready for analysis.

```{r}
# Remove the Student ID and Grade level variables:
transreas.responses <- subset(transreas, select = -c(Student, Grade))

# Check descriptive statistics:
summary(transreas.responses)
```

Now, we are ready to run the dichotomous Rasch model on the transitive reasoning data. We will use the tam() function to run the model and store the results in an object called dichot.transreas:
```{r results='hide'}
# Running the Dichotomous Rasch Model
dichot.transreas <- tam(transreas.responses)
```

For brevity, we do not include all of the output from the model function in this book. However, after you run the model, you should see some output in the R console that includes information about each iteration in the estimation process.

After you run the tam() function, your console will show estimates of item parameters (item difficulty locations), reliability estimates, and a summary of the time in which the analysis was completed. We will explore each of these results in more detail later.

### Overall Model Summary

The next step is to request a summary of the model estimation results in order to begin to understand the results from the analysis.

Request a summary of the dichotomous Rasch model results by applying the summary() function to the model object:

```{r}
# Request a summary of the model results
summary(dichot.transreas)
```

From these results, we suggest taking a quick look at the Item Parameters as reported in the table labeled "Item Parameters in IRT parameterization."
Because we ran a Rasch model, the alpha (discrimination) parameters are fixed to a constant value of 1. The "beta" parameters are the item locations on the latent variable. We will explore the item locations in more detail later in the analysis.

### Plot a Wright Map to visualize item and person locations:

A useful feature of Rasch models is that when there is acceptable fit between the model and the data (discussed in detail in in Chapter 3), it is possible to visualize and compare item and person locations on a single linear continuum. 

Professor Bejamin D. Wright popularized an approach to displaying Rasch model results on a linear continuum, and this technique has been widely adopted by Rasch measurement researchers across disciplines. In his honor, these displays are often called "Wright Maps." In the literature, they are also referred to as "Variable Maps" (XXXX).

As the next step in our analysis, we will use the WrightMap package to create a Wright Map from our model results. We will create the plot using the function IRT.WrightMap(). We will set the option for displaying threshold labels as "FALSE" for now, because we are working with dichotomous data. The following code will produce a Wright Map for the transitive reasoning data:
```{r}
# Plot the variable map 
IRT.WrightMap(dichot.transreas,show.thr.lab=FALSE)
```

[ADD MORE DETAILS ABOUT THE WRIGHT MAP INTERPRETATION]

Even though it is not appropriate to fully interpret item and person locations on the logit scale until there is evidence of acceptable model-data fit, 
we recommend examining the Wright Map during the preliminary stages of an item analysis to get a general sense of the model results and to identify any potential scoring or data entry errors.

We will return to this display for more exploration after checking for acceptable model-data fit, among other psychometric properties.


### Examine Item Parameters:

Next, let's examine the item parameters in detail.

#*#*#* STOPPED EDITING HERE ON 12/6/2020 #*#*#*#*#

```{r}
difficulty <- dichot.transreas$xsi
head(difficulty) 
mean(difficulty$xsi) # The mean difficulty of the tasks is -1.936
sd(difficulty$xsi) # The standard deviation for task difficulty is 1.56
mean(difficulty$se.xsi) # The mean Standard Error for task difficulty is 0.171.
```

- The 'xsi' column denotes the item difficulty in a logit scale; 'se.xsi' is the standard error for each item. The standard error is an estimate of the precision of the item difficulty estimates, where larger standard errors indicate less precise estimates.

- Since the `xsi` indicates the item difficulty, higher values indicate more-difficult items (higher levels of the construct are required for a positive response). For instance, item 9 is the hardest item ( `xsi` = 1.00), whereas item 6 is the easiest item (`xsi` = -4.07).

We can also visualize the item difficulty using a simple histogram
```{r}
hist(difficulty$xsi,breaks=10) 
```

Now let's calculate the Item fit statistics
```{r}
Di_Item_fit <- tam.fit(Di_Rasch_model) 
head(Di_Item_fit)
```

### Person Parameters
Get the person ability by using `tam.wle` function

```{r}
Person_ability <- tam.wle(Di_Rasch_model)
# View(Person_ability)
# In the data frame above, the `theta` is the person's ability measure on a logit scale.
mean(Person_ability$theta) # The average ability for test-taker is -0.056.
sd(Person_ability$theta) # The standard deviation for test-taker ability measures is 1.28.
mean(Person_ability$error) # The mean Standard Error for test-taker ability measures is 1.023.
``` 

Visualize the Person ability measures using a simple histogram
```{r}
hist(Person_ability$theta)
```

Calculate the Person fit statistics
```{r}
Di_Person_fit <- tam.personfit(Di_Rasch_model) 
head(Di_Person_fit)
```

### Summarize the results in a table
> For Item Calibration Table

```{r}
# Set up the contents for table2
Table2 <- data.frame()
Table2 <- setNames(data.frame(matrix(ncol = 8, nrow = 10)), c("TaskID", "PropCorrect", "Delta","SE","Outfit","Outfit_P","Infit","Infit_P"))
# Calculate the proportion correct (you can also type in these values by hand from the previous outcome)
TaskCorrect <- apply(Di_Rasch_data, 2, sum)
PropCorrect <- percent(TaskCorrect/425)
Table2$TaskID <- 1:10
Table2$PropCorrect <- PropCorrect
Table2$Delta <- difficulty$xsi
Table2$SE <- difficulty$se.xsi
Table2$Outfit <- Di_Item_fit[["itemfit"]][["Outfit"]]
Table2$Outfit_P <- Di_Item_fit[["itemfit"]][["Outfit_p"]]
Table2$Infit <- Di_Item_fit[["itemfit"]][["Infit"]]
Table2$Infit_P <- Di_Item_fit[["itemfit"]][["Infit_p"]]
# Sort the table 2 by Item difficulty
Table2 <- Table2[order(-PropCorrect),]
```

> For Person calibration table

```{r}
# Set up the contents for table3
Table3 <- data.frame()
Table3 <- setNames(data.frame(matrix(ncol = 8, nrow = 425)), c("TestTakerID", "PropCorrect", "Theta","SE","Outfit","Outfit_t","Infit","Infit_t"))
# Calculate the Proportion Correct
Person_Score <- rowSums(Di_Rasch_data, na.rm=FALSE) 
Person_PropCorrect <- Person_Score/10
Table3$TestTakerID <- 1:425
Table3$PropCorrect <- Person_PropCorrect
Table3$Theta <- Person_ability$theta
Table3$SE <- Person_ability$error
Table3$Outfit <- Di_Person_fit$outfitPerson
Table3$Outfit_t <- Di_Person_fit$outfitPerson_t
Table3$Infit <- Di_Person_fit$infitPerson
Table3$Infit_t <- Di_Person_fit$infitPerson_t 
# Note here the TAM package only report t value instead of p value. However, you still can calculate that by yourself if you need it.
```

## Runing the Dichotomous Rasch Model with JML Method

I just noticed that we can also use the **tam.jml()** function to run the Dichotomous Rasch Model using the JML method, which is the same estimation method with the Winstep software. Most of your code will be the same with the previous example. This section will give to an example to run the Dichotomous Rasch Model with JML Method.

```{r results='hide'}
# Running the Dichotomous Rasch Model use tam.jml() function
Di_Rasch_model_jml <- tam.jml(Di_Rasch_data)
```

### Overall Model Summary

```{r}
# Check the summary
summary(Di_Rasch_model_jml)
# Plot the person-item map
difficulty <- Di_Rasch_model_jml$xsi
wrightMap(Di_Rasch_data,difficulty)
```

### Item Parameters
First of all, let's pull out the item parameters from your model.
```{r}
difficulty <- Di_Rasch_model_jml$xsi
head(difficulty) 
mean(difficulty) # The mean difficulty of the tasks is -1.985
sd(difficulty) # The standard deviation for task difficulty is 1.595
```

- The 'xsi' column denotes the item difficulty in a logit scale; 'se.xsi' is the standard error for each item. The standard error is an estimate of the precision of the item difficulty estimates, where larger standard errors indicate less precise estimates.

- Since the `xsi` indicates the item difficulty, higher values indicate more-difficult items (higher levels of the construct are required for a positive response). For instance, item 9 is the hardest item ( `xsi` = 1.00), whereas item 6 is the easiest item (`xsi` = -4.07).

We can also visualize the item difficulty using a simple histogram
```{r}
hist(difficulty) 
```

Now let's calculate the Item fit statistics
```{r}
Di_fit <- tam.jml.fit(Di_Rasch_model_jml) 
head(Di_fit$fit.item)
```

### Person Parameters and fit statistics

```{r}
head(Di_fit$fit.person)
# View Person Scores and Person ability, which is the "theta"
# In the data frame above, the `theta` is the person's ability measure on a logit scale.
mean(Di_Rasch_model_jml$theta) # The average ability for test-taker is almost zero.
sd(Di_Rasch_model_jml$theta) # The standard deviation for test-taker ability measures is 1.472.
mean(Di_Rasch_model_jml[["errorWLE"]]) # The mean Standard Error for test-taker ability measures is 1.025.
``` 

Visualize the Person ability measures using a simple histogram
```{r}
hist(Di_Rasch_model_jml$theta)
```

### Compare the results of estimated parameters between JML and MML method

```{r}
plot(Di_Rasch_model_jml$xsi, Di_Rasch_model$xsi$xsi, pch=16,
     xlab=expression(paste(xi[i], "(JML)")),
     ylab=expression(paste(xi[i], "(MML)")),
     main="Item Parameter Estimate Comparison")
lines(c(-5,5), c(-5,5), col="gray" )
```


## Example APA-Style Results Write-Up (Transitive Reasoning Test)

Table 1 presents a summary of the results from the analysis of the transitive reasoning data [Sijtsma and Molenaar,2002](https://methods.sagepub.com/book/introduction-to-nonparametric-item-response-theory) using the dichotomous Rasch model ([Rasch, 1960](https://eric.ed.gov/?id=ED419814)). Specifically, the calibration of test participants (*N* = 425) and Tasks (*N* = 10) are summarized using average logit-scale calibrations, standard errors, and model-data fit statistics. Examination of the results indicates that, on average, the task takers were located higher on the logit scale (*M* = -0.056,*SD* = 1.281), compared to Tasks (*M* = -1.936, *SD* = 1.281). This finding suggests that the items were relatively easy for the sample of kids who participated in this transitive reasoning test. However, average values of the Standard Error (*SE*) are slightly higher for Kids (*M* = 1.023) than Tasks (*M* = 0.17), indicating that there may be some issues related to targeting for some of the Kids who participated in the assessment. Average values of model-data fit statistics indicate overall adequate fit to the model, with average Infit and Outfit mean square statistics around 1.00, [and average standardized Infit and Outfit statistics near the expected value of 0.00 when data fit the model.] **This sentence needs rephrase.** This finding of adequate fit to the model supports the interpretation of item and person calibrations on the logit scale as indicators of their locations on the latent variable measured by the test.

```{r}
# Print the table2 in a neat way
knitr::kable(
  Table2[,-1], booktabs = TRUE,
  caption = 'Item Calibration'
)
```

Table 2.1 includes detailed results for the 10 Task items included in the Transitive Reasoning test. For each item, the proportion of correct responses is presented, followed by the logit-scale calibration (*δ*), SE, and model-data fit statistics. Examination of these results indicates that Task 9 was the most difficult (*Proportion Correct*  = 30.12%; *δ* = 1.00 ; *SE*  = .11), followed by Task 10 (Proportion Correct  = 52%; *δ* = -.09; *SE*  = 0.11). The easiest item was Task 6(*Proportion Correct* = 97.41%; *δ* = -4.07; *SE* = 0.31).

```{r}
# Print the table3 in a neat way
knitr::kable(
  head(Table3,10), booktabs = TRUE,
  caption = 'Person Calibration'
)
```

Table 3 includes detailed results for first 10 test takers who participated in the Transitive Reasoning Test. For each participant, the proportion of correct responses is presented, followed by their logit-scale measure (*θ*), *SE*, and model-data fit statistics. Examination of these results indicates that around 51 participants has the highest score (*Proportion Correct* = 100%; *θ* = 2.347; *SE* = 1.762). The lowest score test taker was *ID.148* (*Proportion Correct* = 10%; *θ* = -4.52; *SE* = 1.03).

```{r}
# Plot the variable-Map
IRT.WrightMap(Di_Rasch_model,show.thr.lab=FALSE)
```

Figure 1 illustrates the calibrations of the Participants and Items on the logit scale that represents the latent variable. The calibrations shown in this figure correspond to the calibrations presented in Table 2 and Table 3 for items and persons, respectively. The rightmost column (Measure) shows the logit scale. Higher numbers correspond to higher levels of achievement (for persons) and higher levels of difficulty (for items), and lower numbers correspond to lower achievement and less difficulty, respectively, for persons and items. Next, Respondents on the latent variable are illustrated using the histogram. Examination of the histogram indicates a wide spread of achievement levels, with most students grouped near the middle of the logit scale (*θ* = 0.00). Next, Task locations on the logit scale are plotted on the right side. Examination of the Tasks plotting indicates a similar overall spread as the participants measures. However, the Tasks appear somewhat clustered at the lower half of the logit scale, without many items appearing above average (*θ* >= 0.00). This lack of moderate-difficulty items may have contributed to the somewhat large SE values for students with middle-range calibrations.  

## Exericise

Use the simulated data to run Dichotomous Rasch Model using TAM package.

[The Data could be either attached to this site or Blackboard]


## Supplementary Learning Materials

[Rasch Estimation Demonstration Spreadsheet](https://www.rasch.org/moulton.xls)

[Li, Y. Using the open-source statistical language R to analyze the dichotomous Rasch model. Behavior Research Methods 38, 532–541 (2006). https://doi.org/10.3758/BF03192809](https://link.springer.com/content/pdf/10.3758/BF03192809.pdf)

[Rasch, G. (1960/1980). Probabilistic models for some intelligence and attainment tests.(Copenhagen, Danish Institute for Educational Research), expanded edition (1980) with foreword and afterword by B.D. Wright. Chicago: The University of Chicago Press.](https://eric.ed.gov/?id=ED419814)

[Wright, B. D., & Masters, G. N. (1982). Rating Scale Analysis: Rasch Measurement. Chicago, IL: MESA Press.](https://pdfs.semanticscholar.org/8083/5035228bc338840ed6c67e879b4bcef11e07.pdf?_ga=2.216101590.1797749273.1596845271-1703835138.1596845271)


