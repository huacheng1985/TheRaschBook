# Dichotomous Rasch Models {#Dich_Rasch_model}

This chapter provides a basic overview of the dichotomous Rasch model, along with guidance for analyzing data using the dichotomous Rasch model with R. At the end of the chapter, a challenge exercise and resources for further study are presented.

## Dichotomous Rasch Model

The dichotomous Rasch model (Rasch, 1960) is the simplest model in the Rasch family of models (Wright & Mok, 2004). It was designed for use with ordinal data that are scored in two categories (usually 0 or 1). The dichotomous Rasch model uses sum scores from these ordinal responses to calculate interval-level estimates that represent person locations (i.e., person ability or person achievement) and item locations (i.e., the difficulty to provide a correct or positive response) on a linear scale that represents the latent variable (the log-odds or "logit" scale). The difference between person and item locations can be used to calculate the probability for a correct or positive response (x = 1), rather than an incorrect or negative response (x = 0).

### Model Equation

The equation for the dichotomous Rasch model can be expressed in log-odds form as follows:

$$\ln_{}{}\left[\frac{\phi_{n i 1}}{\phi_{n i 0}}\right]=\theta_{n}-\delta_{i}$$

The Rasch model predicts the probability of person *n* on item *i* providing a correct or positive (x = 1), rather than an incorrect or negative (x = 0) response, given a person’s “ability” (*θn*) and an item’s difficulty (*δi*), as expressed on the logit scale.

### Model Requirements

Estimates that are calculated using the dichotomous Rasch model can only be meaningfully interpreted if there is evidence that the data approximate the requirements for the model. Key among dichotomous Rasch model requirements are the following:

* Unidimensionality: A single latent variable is sufficient to explain most of the variation in item responses
* Local independence: After controlling for the latent variable, there is no meaningful relationship between the responses to individual items
* Person-invariant item estimates: Item locations do not depend on (i.e., are independent from) the persons whose responses are used to estimmate them
* Item-invariant person estimates: Person locations  do not depend on (i.e., are independent from) the items used to estimate them

Evidence that data approximate these requirements provides support for the meaningful interpretation and use of item and person estimates on the logit scale as indicators of item and person locations on the latent variable. In practice, many analysts evaluate these requirements using indicators of model-data fit for the facets in a Rasch model (in this case, items and persons). In the current chapter, we provide some basic code for calculating fit indices for items and persons. We explore this topic in more detail in Chapter 3.


## R-Lab: Running Dichotomous Rasch Model in R
### Understand the Data set
In this example,we will be working with data from a transitive reasoning test, which is a reasoning test related to relationships among physical objects. The transitive reasoning data were collected from a one-on-one interactive assessment in which an experimenter presented students with a set of objects, such as sticks, balls, cubes, and discs. The following description is given in [Sijtsma and Molenaar](https://methods.sagepub.com/book/introduction-to-nonparametric-item-response-theory) (2002), pp. 31-32:

> The items for transitive reasoning had the following structure. A typical item used three sticks, here denoted A, B, and C, of different length, denoted Y, such that YA < YB < YC. The actual test taking had the form of a conversation between experimenter and child in which the sticks were identified by their colors rather than letters. First, sticks A and B were presented to a child, who was allowed to pick them up and compare their lengths, for example, by placing them next to each other on a table. 

> Next, sticks B and C were presented and compared. Then all three sticks were displayed in a random order at large mutual distances so that their length differences were imperceptible, and the child was asked to infer the relation between sticks A and C from his or her knowledge of the relationship in the other two pairs.

The transitive reasoning items varied in terms of the property students were asked to reason about (length, weight, area). The tasks also varied in terms of the number of items students were asked to reason about, and whether the tasks involved equalities, inequalities, or a mixture of equalities and inequalities. The characteristics of the transitive reasoning data are summarized in the following table:

|Task|Property|Format|Objects|Measures|
| :--- | :---- |  :---- | :---- | :---- |
|1|Length|YA > YB > YC|Sticks|12, 11.5, 11 (cm)|
|2|Length|YA = YB = YC = YD |Tubes|12 (cm)|
|3|Weight|YA > YB > YC|Tubes|45, 25, 18 (g)|
|4|Weight|YA = YB = YC = YD|Cubes|65 (g)|
|5|Weight|YA < YB < YC|Balls|40, 50, 70 (g)|
|6|Area|YA > YB> YC|Discs|2.5, 7, 6.5 (diameter; cm)|
|7|Length|YA > YB = YC|Sticks|28.5, 27.5, 27.5 (cm)|
|8|Weight|YA >YB = YC|Balls|65, 40, 40 (g)|
|9|Length|YA = YB = YC = YD|Sticks|12.5, 12.5, 13, 13 (cm)|
|10|Weight|YA = YB < YC = YD|Balls|60, 60, 100, 100 (g)|


### Prepare the R Packages for analysis
In this session, we use the "TAM" package to do the Rasch Analysis. TAM is actually the abbreviation for *"Test Analysis Modules"*. Different from the *Winstep* software, it applies marginal maximum likelihood estimation (MMLE) instead of joint maximum likelihood estimation (JMLE).
Note: Because of the different way of the estimation, the result might be slightly different with your *Winstep* outcomes.

```{r message=FALSE, warning=FALSE}
# Load the R package that we need for this analysis
library("TAM") # For Dichotomous Rasch Analysis
library("WrightMap") # For plotting the variable map
library("Hmisc") # For descriptive data analysis
library("formattable") # For format number as percentage
```

### Import the data & Running descriptive analysis
```{r message=FALSE}
library(readr) # Import the data from your computer
transreas <- read_csv("transreas.csv")
```
Then,we run a descriptive analysis on our data.
```{r}
# Use the summary() function to overview the data
summary(transreas)
# From the result, we can see there is no missing data for each variable. And we can also get a general idea on the range of the grades (from 2 to 6), Min, Max, Median for each task. 
# You can also use describe() function as an alternative approach
describe(transreas)
# The Mean for each task in this table is the Proportion Correct statistic for each item (item difficulty estimate for Classical Test Theory)
```

### Runing the Dichotomous Rasch Model with CML Method
To run the dichotomous Rasch Model using the TAM package, we only need students' item-level responses. So the first and second column of our data are not needed. 
```{r}
# Trim the data
Di_Rasch_data <- transreas[,c(-1,-2)]
head(Di_Rasch_data) # Take a look
```

```{r results='hide'}
# Running the Dichotomous Rasch Model
Di_Rasch_model <- tam(Di_Rasch_data)
```

### Overall Model Summary

```{r}
# Check the summary
summary(Di_Rasch_model)
# Plot the variable map 
IRT.WrightMap(Di_Rasch_model,show.thr.lab=FALSE)
```

### Item Parameters
First of all, let's pull out the item parameters from your model.
```{r}
difficulty <- Di_Rasch_model$xsi
head(difficulty) 
mean(difficulty$xsi) # The mean difficulty of the tasks is -1.936
sd(difficulty$xsi) # The standard deviation for task difficulty is 1.56
mean(difficulty$se.xsi) # The mean Standard Error for task difficulty is 0.171.
```

- The 'xsi' column denotes the item difficulty in a logit scale; 'se.xsi' is the standard error for each item. The standard error is an estimate of the precision of the item difficulty estimates, where larger standard errors indicate less precise estimates.

- Since the `xsi` indicates the item difficulty, higher values indicate more-difficult items (higher levels of the construct are required for a positive response). For instance, item 9 is the hardest item ( `xsi` = 1.00), whereas item 6 is the easiest item (`xsi` = -4.07).

We can also visualize the item difficulty using a simple histogram
```{r}
hist(difficulty$xsi,breaks=10) 
```

Now let's calculate the Item fit statistics
```{r}
Di_Item_fit <- tam.fit(Di_Rasch_model) 
head(Di_Item_fit)
```

### Person Parameters
Get the person ability by using `tam.wle` function

```{r}
Person_ability <- tam.wle(Di_Rasch_model)
# View(Person_ability)
# In the data frame above, the `theta` is the person's ability measure on a logit scale.
mean(Person_ability$theta) # The average ability for test-taker is -0.056.
sd(Person_ability$theta) # The standard deviation for test-taker ability measures is 1.28.
mean(Person_ability$error) # The mean Standard Error for test-taker ability measures is 1.023.
``` 

Visualize the Person ability measures using a simple histogram
```{r}
hist(Person_ability$theta)
```

Calculate the Person fit statistics
```{r}
Di_Person_fit <- tam.personfit(Di_Rasch_model) 
head(Di_Person_fit)
```

### Summarize the results in a table
> For Item Calibration Table

```{r}
# Set up the contents for table2
Table2 <- data.frame()
Table2 <- setNames(data.frame(matrix(ncol = 8, nrow = 10)), c("TaskID", "PropCorrect", "Delta","SE","Outfit","Outfit_P","Infit","Infit_P"))
# Calculate the proportion correct (you can also type in these values by hand from the previous outcome)
TaskCorrect <- apply(Di_Rasch_data, 2, sum)
PropCorrect <- percent(TaskCorrect/425)
Table2$TaskID <- 1:10
Table2$PropCorrect <- PropCorrect
Table2$Delta <- difficulty$xsi
Table2$SE <- difficulty$se.xsi
Table2$Outfit <- Di_Item_fit[["itemfit"]][["Outfit"]]
Table2$Outfit_P <- Di_Item_fit[["itemfit"]][["Outfit_p"]]
Table2$Infit <- Di_Item_fit[["itemfit"]][["Infit"]]
Table2$Infit_P <- Di_Item_fit[["itemfit"]][["Infit_p"]]
# Sort the table 2 by Item difficulty
Table2 <- Table2[order(-PropCorrect),]
```

> For Person calibration table

```{r}
# Set up the contents for table3
Table3 <- data.frame()
Table3 <- setNames(data.frame(matrix(ncol = 8, nrow = 425)), c("TestTakerID", "PropCorrect", "Theta","SE","Outfit","Outfit_t","Infit","Infit_t"))
# Calculate the Proportion Correct
Person_Score <- rowSums(Di_Rasch_data, na.rm=FALSE) 
Person_PropCorrect <- Person_Score/10
Table3$TestTakerID <- 1:425
Table3$PropCorrect <- Person_PropCorrect
Table3$Theta <- Person_ability$theta
Table3$SE <- Person_ability$error
Table3$Outfit <- Di_Person_fit$outfitPerson
Table3$Outfit_t <- Di_Person_fit$outfitPerson_t
Table3$Infit <- Di_Person_fit$infitPerson
Table3$Infit_t <- Di_Person_fit$infitPerson_t 
# Note here the TAM package only report t value instead of p value. However, you still can calculate that by yourself if you need it.
```

## Runing the Dichotomous Rasch Model with JML Method

I just noticed that we can also use the **tam.jml()** function to run the Dichotomous Rasch Model using the JML method, which is the same estimation method with the Winstep software. Most of your code will be the same with the previous example. This section will give to an example to run the Dichotomous Rasch Model with JML Method.

```{r results='hide'}
# Running the Dichotomous Rasch Model use tam.jml() function
Di_Rasch_model_jml <- tam.jml(Di_Rasch_data)
```

### Overall Model Summary

```{r}
# Check the summary
summary(Di_Rasch_model_jml)
# Plot the person-item map
difficulty <- Di_Rasch_model_jml$xsi
wrightMap(Di_Rasch_data,difficulty)
```

### Item Parameters
First of all, let's pull out the item parameters from your model.
```{r}
difficulty <- Di_Rasch_model_jml$xsi
head(difficulty) 
mean(difficulty) # The mean difficulty of the tasks is -1.985
sd(difficulty) # The standard deviation for task difficulty is 1.595
```

- The 'xsi' column denotes the item difficulty in a logit scale; 'se.xsi' is the standard error for each item. The standard error is an estimate of the precision of the item difficulty estimates, where larger standard errors indicate less precise estimates.

- Since the `xsi` indicates the item difficulty, higher values indicate more-difficult items (higher levels of the construct are required for a positive response). For instance, item 9 is the hardest item ( `xsi` = 1.00), whereas item 6 is the easiest item (`xsi` = -4.07).

We can also visualize the item difficulty using a simple histogram
```{r}
hist(difficulty) 
```

Now let's calculate the Item fit statistics
```{r}
Di_fit <- tam.jml.fit(Di_Rasch_model_jml) 
head(Di_fit$fit.item)
```

### Person Parameters and fit statistics

```{r}
head(Di_fit$fit.person)
# View Person Scores and Person ability, which is the "theta"
# In the data frame above, the `theta` is the person's ability measure on a logit scale.
mean(Di_Rasch_model_jml$theta) # The average ability for test-taker is almost zero.
sd(Di_Rasch_model_jml$theta) # The standard deviation for test-taker ability measures is 1.472.
mean(Di_Rasch_model_jml[["errorWLE"]]) # The mean Standard Error for test-taker ability measures is 1.025.
``` 

Visualize the Person ability measures using a simple histogram
```{r}
hist(Di_Rasch_model_jml$theta)
```

### Compare the results of estimated parameters between JML and MML method

```{r}
plot(Di_Rasch_model_jml$xsi, Di_Rasch_model$xsi$xsi, pch=16,
xlab=expression(paste(xi[i], "(JML)")),
ylab=expression(paste(xi[i], "(MML)")),
main="Item Parameter Estimate Comparison")
lines(c(-5,5), c(-5,5), col="gray" )
```


## Example APA-Style Results Write-Up (Transitive Reasoning Test)

Table 1 presents a summary of the results from the analysis of the transitive reasoning data [Sijtsma and Molenaar,2002](https://methods.sagepub.com/book/introduction-to-nonparametric-item-response-theory) using the dichotomous Rasch model ([Rasch, 1960](https://eric.ed.gov/?id=ED419814)). Specifically, the calibration of test participants (*N* = 425) and Tasks (*N* = 10) are summarized using average logit-scale calibrations, standard errors, and model-data fit statistics. Examination of the results indicates that, on average, the task takers were located higher on the logit scale (*M* = -0.056,*SD* = 1.281), compared to Tasks (*M* = -1.936, *SD* = 1.281). This finding suggests that the items were relatively easy for the sample of kids who participated in this transitive reasoning test. However, average values of the Standard Error (*SE*) are slightly higher for Kids (*M* = 1.023) than Tasks (*M* = 0.17), indicating that there may be some issues related to targeting for some of the Kids who participated in the assessment. Average values of model-data fit statistics indicate overall adequate fit to the model, with average Infit and Outfit mean square statistics around 1.00, [and average standardized Infit and Outfit statistics near the expected value of 0.00 when data fit the model.] **This sentence needs rephrase.** This finding of adequate fit to the model supports the interpretation of item and person calibrations on the logit scale as indicators of their locations on the latent variable measured by the test.

```{r}
# Print the table2 in a neat way
knitr::kable(
  Table2[,-1], booktabs = TRUE,
  caption = 'Item Calibration'
)
```

Table 2.1 includes detailed results for the 10 Task items included in the Transitive Reasoning test. For each item, the proportion of correct responses is presented, followed by the logit-scale calibration (*δ*), SE, and model-data fit statistics. Examination of these results indicates that Task 9 was the most difficult (*Proportion Correct*  = 30.12%; *δ* = 1.00 ; *SE*  = .11), followed by Task 10 (Proportion Correct  = 52%; *δ* = -.09; *SE*  = 0.11). The easiest item was Task 6(*Proportion Correct* = 97.41%; *δ* = -4.07; *SE* = 0.31).

```{r}
# Print the table3 in a neat way
knitr::kable(
  head(Table3,10), booktabs = TRUE,
  caption = 'Person Calibration'
)
```

Table 3 includes detailed results for first 10 test takers who participated in the Transitive Reasoning Test. For each participant, the proportion of correct responses is presented, followed by their logit-scale measure (*θ*), *SE*, and model-data fit statistics. Examination of these results indicates that around 51 participants has the highest score (*Proportion Correct* = 100%; *θ* = 2.347; *SE* = 1.762). The lowest score test taker was *ID.148* (*Proportion Correct* = 10%; *θ* = -4.52; *SE* = 1.03).

```{r}
# Plot the variable-Map
IRT.WrightMap(Di_Rasch_model,show.thr.lab=FALSE)
```

Figure 1 illustrates the calibrations of the Participants and Items on the logit scale that represents the latent variable. The calibrations shown in this figure correspond to the calibrations presented in Table 2 and Table 3 for items and persons, respectively. The rightmost column (Measure) shows the logit scale. Higher numbers correspond to higher levels of achievement (for persons) and higher levels of difficulty (for items), and lower numbers correspond to lower achievement and less difficulty, respectively, for persons and items. Next, Respondents on the latent variable are illustrated using the histogram. Examination of the histogram indicates a wide spread of achievement levels, with most students grouped near the middle of the logit scale (*θ* = 0.00). Next, Task locations on the logit scale are plotted on the right side. Examination of the Tasks plotting indicates a similar overall spread as the participants measures. However, the Tasks appear somewhat clustered at the lower half of the logit scale, without many items appearing above average (*θ* >= 0.00). This lack of moderate-difficulty items may have contributed to the somewhat large SE values for students with middle-range calibrations.  

## Exericise

Use the simulated data to run Dichotomous Rasch Model using TAM package.

[The Data could be either attached to this site or Blackboard]


## Supplementary Learning Materials

[Rasch Estimation Demonstration Spreadsheet](https://www.rasch.org/moulton.xls)

[Li, Y. Using the open-source statistical language R to analyze the dichotomous Rasch model. Behavior Research Methods 38, 532–541 (2006). https://doi.org/10.3758/BF03192809](https://link.springer.com/content/pdf/10.3758/BF03192809.pdf)

[Rasch, G. (1960/1980). Probabilistic models for some intelligence and attainment tests.(Copenhagen, Danish Institute for Educational Research), expanded edition (1980) with foreword and afterword by B.D. Wright. Chicago: The University of Chicago Press.](https://eric.ed.gov/?id=ED419814)

[Wright, B. D., & Masters, G. N. (1982). Rating Scale Analysis: Rasch Measurement. Chicago, IL: MESA Press.](https://pdfs.semanticscholar.org/8083/5035228bc338840ed6c67e879b4bcef11e07.pdf?_ga=2.216101590.1797749273.1596845271-1703835138.1596845271)


