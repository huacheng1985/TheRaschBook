# Model-Data Fit analyses {#MD_fit}

This chapter provides readers with an overview of several popular model-data fit analyses that can be used to inform the interpretation and use of Rasch model estimates. The chapter begins with a brief overview of model-data fit analysis from the perspective of Rasch measurement theory (Rasch, 1960). Then, we demonstrate several tools that analysts can use to evaluate model-data fit for Rasch models using R. As we did in Chapter 2, we use a dataset from a transitive reasoning assessment that was originally presented by Sijtsma and Molenaar (2002) to illustrate the application and interpretation of several model-data fit indices. The chapter concludes with a challenge exercise and resources for further study.

In using this chapter, we encourage readers to note that model-data fit analysis in modern measurement theory in general, as well as in the context of Rasch modles, is nuanced. The interpretation and use of model-data fit indices varies across assessment contexts that have unique purposes and consequences. As a result, it is critical that analysts consider the unique context in which they are evaluating model-data fit when they interpret the results from model-data fit analyses, rather than relying on previously published critical values or "rules-of-thumb" to interpret the results. In this chapter, we demonstrate several techniques that analysts can use to evaluate model-data fit within the Rasch measurement theory framework. However, our presentation is not exhaustive and there are many other methods that can be used to supplement those that we demonstrate here. 

## Overview of Model-Data Fit Analyses from the Perspective of Rasch Measurement Theory

In the context of Rasch measurement theory (Rasch, 1960), model-data fit analyses focus on evaluating the item response patterns associated with individual items, persons, and other elements of an assessment context (e.g., raters) for evidence of adherence to model requirements. Fit analyses help researchers identify individual elements whose responses deviate from what would be expected given adherence to the Rasch model. These deviations can alert researchers to individual elements of an assessment context that warrant additional investigation (e.g., in using qualitative analyses), revision (e.g., revisions to item text, revision of scoring materials in performance assessments), inform theory about the construct, and identify areas for future research. This individual-focused approach to evaluating model-data fit in which model requirements are used as a framework against which to *evaluate* response patterns stands in contrast to model-data fit approaches that often accompany other modern measurement approaches whose goal is typically to identify item response theory models that reflect the characteristics of a given dataset at a group level. 

### Example Data: Transitive Reasoning
We are going to practice evaluating model-data fit using the Transitive Reasoning assessment data that we explored in Chapter 2; these data were originally presented by Sijtsma and Molenaar (2002). Please see Chapter 2 for a detailed description of the dataset. 

### The eRm package
In this chapter, we will use the "eRm" package [(Mair & Hatzinger, 2007)](https://cran.r-project.org/web/packages/TAM/TAM.pdf) to conduct model-data fit analyses. It is possible to evaluate model-data fit using other R packages, including the other packages that we have included in this book. We selected eRm as the primary package for the analyses in the current chapter because it includes functions for evaluating model-data fit that are relatively straightforward to apply and interpret. Please note that the "eRm" package  uses the Conditional Maximum Likelihood Estimation (CMLE) method to estimate Rasch model parameters. As a result, estimates from the eRm package are not directly comparable to estimates obtained using other estimation methods.

To get started with the TAM package, install and load it into your R environment using the following code:

```{r}
citation("eRm")
install.packages("eRm")
library("eRm")
```

** SW paused editing here on December 22 ***


```{r message=FALSE}
# Import the data
transreas <- read.csv("transreas.csv") 
summary(transreas)
```

### Trim the data
Similar to the "TAM" package, we only need the responses to run the Dichotomous Rasch model with the "eRm" package. To get started, we need to remove the first two columns from the dataframe.

```{r}
# Trim the data
Di_Rasch_data <- transreas[ ,c(-1,-2)]
head(Di_Rasch_data,10) # Take a look
```
### Running Dichotomous Rasch Model with "eRm" package
We will use the "RM" function to run the Rasch dichotomous model. This function computes the parameter estimates of a Rasch model for binary item responses by using CML estimation.

```{r}
# Running the Dichotomous Rasch Model
Di_Rasch_model <- RM(Di_Rasch_data)
# Check the Overall model summary
summary(Di_Rasch_model)
# To achieve the Item difficulty
item.diff <- Di_Rasch_model$betapar * -1
item.diff
# Use the summary() function to get a quick numeric summary of the item parameters
summary(item.diff)
# We can see that the average item difficulty value is 0.00 logits, and item difficulties range from -2.18 to 1.04 logits.
# We can also look at the standard errors for the item locations:
item.se <- Di_Rasch_model$se.beta
item.se
summary(item.se)
```
Note the Estimate in this output indicates the easiness of the Item. This is exactly the item difficulty, but in the opposite direction. The higher the value of this parameter, the easier the item is compared to the other items. You can multiply this value by -1 to get item difficulty.

## Model-data fit Analysis in R

The Model-data fit in the context of Rasch is different from other IRT models. Other IRT approach is focus on finding the _best model_ to fit the data. However, the Rasch approach focuses on diagnosing departures from model expectations. Within the Rasch framework, the model is viewed as an _"ideal type"_. It is a theoretical mathematical description of what measurement looks like. Its fit statistics summarize discrepancies between observations and expectations to help researchers improve their measurement procedures.

### Reliability Indices in Rasch Measurement {#Reliability}

> Definition of reliability in the 2014 Test Standards

>  The general notion of reliabilty/precision is defined in terms of consistency over replications of the testing procedure. Reliability/precision is high if  the scores for each person are consistent over replications of the testing procedure and is low if the scores are not consistent over replications. (p. 35, emphasis added)

From a Rasch perspective, the focus for reliability analyses is on **ordering** and **separation** on the logit scale. There are two major indices calculated for items and persons: one is the reliability of separation, and the other is the Chi-Square separation statistic.

The Rasch reliability of separation is calculated for each facet in the model (e.g., items & persons), and it is an estimate of **how well we can differentiate** individual items, persons, or other elements on the latent variable. It is conceptually related to Cronbach's alpha coefficient. The interpretation is the same when data fit the model. The statistic ranges from 0 to 1. 

#### Reliability of Person Separation
Calculated using a ratio of true (adjusted) variance to observed variance for persons:
$$Rel_{p}=\left(SA_{P}^{2}\right) /\left(SD_{P}^{2}\right)$$
Where:
	  *SA^2^~P~* : Adjusted person variability;
    Calculated by subtracting error variance for persons from total person variance: $$ SA_{P}^{2} = SD_{P}^{2} - SE_{P}^{2}$$
	  *SD^2^~P~* : Total person variance

#### Calculate the Reliability of Person Separation
The "eRm" package provides function "SepRel" to calculate the person separation reliability. This function calculates the proportion of person variance that is not due to error. The concept of person separation reliability is very similar to reliability indices such as Cronbach's α.

```{r}
# Get the person parameter first by using the "person.parameter" function
person_pa <- person.parameter(Di_Rasch_model)
# Calculate the Reliability of Person Separation
summary(SepRel(person_pa))
```

#### Reliability of Item Separation
Calculated using a ratio of true (adjusted) variance to observed variance for Items:
$$Rel_{I}=\left(SA_{I}^{2}\right) /\left(SD_{I}^{2}\right)$$
Where:
	  *SA^2^~I~* : Adjusted item variability;
    Calculated by subtracting error variance for Item from total Item          variance: $$ SA_{I}^{2} = SD_{I}^{2} - SE_{I}^{2}$$
	  *SD^2^~I~* : Total Item variance
	  
### Item Information Curve (*IIC*)
Many IRT analyses also look at item information as evidence for precision. This is a statistical summary of the variance of item responses about a certain range on the latent variable. We can use this information to find out **if** and **where** items are providing information about person locations.
IIC is not a major component of Rasch analyses, because the information is the same for all items in the dichotomous model (same shape). This is because the item slope parameter is fixed to 1 for all items, so all of the items discriminate among students the same way.

```{r}
# Use "plotINFO" function for visualizing the item information
plotINFO(Di_Rasch_model)
```

### Summaries of residuals: Infit & Outfit

The most popular Rasch fit statistics for practical purposes are based on summed squared residuals. There are two major categories of residual summary statistics: Unweighted (Outfit) and Weighted (Infit) mean square error (MSE) statistics. Unstandardized (χ2) & standardized versions (Z) are available in most Rasch software programs. In this analysis, we will use the Unstandardized (χ2) version.

#### Outfit Mean Square Error (MSE)

Outfit is the “Unweighted fit” statistic. For items, it is the sum of squared residuals for an item divided by the number of persons who responded to the item. For persons, it is sum of squared residuals for a person divided by the number of items encountered by the person.

The outfit is sensitive to extreme departures from model expectations. Examples: A high-achieving student provides an incorrect response to a very easy item; A low-achieving student provides a correct response to a very difficult item.

#### Infit Mean Square Error (MSE)

Infit is "Information-weighted fit", where "information" means *variance*, such as larger variance for well-targeted observations, or smaller variance for extreme observations.

For items, it is the sum of squared standardized *item residuals*, weighted by variance, divided by the number of persons who responded to the item. For persons, it is the sum of squared standardized *person residuals*, weighted by variance, divided by the number of items the person encountered.

Infit is sensitive to less-extreme unexpected responses compared to outfit. Examples of less-extreme unexpected responses include: A person provides an incorrect response to an item that is just below their achievement level, or a person provides a correct response to an item that is just above their achievement level.

#### Expected values for MSE Fit Statistics

Note that there is much disagreement among measurement scholars about how to classify an infit our outfit statistic as "fitting" or "misfitting." We will talk about this in class. 

However, you should be aware of commonly accepted rule-of-thumb values among Rasch researchers:

- Expected value is about 1.00 when data fit the model

- Less than 1.00: Responses are too predictable; they resemble a Guttman-like (deterministic) pattern (“muted”)

- More than 1.00: Responses are too haphazard (“noisy”); there is too much variation to suggest that the estimate is a good representation of the response pattern

- Some variation is expected, but noisy responses are usually more cause for concern than muted responses

Frequently Used Critical Values for Mean Square Fit Statistics [(Bond & Fox, p. 273, Table 12.7)](https://psycnet.apa.org/record/2007-07586-000)

| *Type of Instrument* | *"Acceptable Range"* | 
| :---:        |    :----:   | 
| Multiple-choice test (high-stakes) | 0.80 – 1.20 | 
| Multiple-choice test (not high-stakes)| 0.70 – 1.30 | 
| Rating scale | 0.60 – 1.40 | 
| Clinical observation | 0.50 – 1.70 | 
| Judgment (when agreement is encouraged) | 0.40 – 1.20| 

*Note: These critical values are a very contentious topic in Rasch measurement!!!

#### Calculate Infit & Outfit for the transitive reasoning data
```{r}
# Calculate the Item fit statistics using "itemfit" function on your person parameter object
Di_itemfit <- itemfit(person_pa)
Di_itemfit
```
This table will give us information about the infit and outfit statistics for each item. Please review our lecture materials for details about the interpretation of these values, noting that we generally expect these statistics to be around 1.00.

```{r}
# Calculate the Person fit statistics
person_fit <- personfit(person_pa)
# Since person_fit is a long list, we can summarize it to get the aggregated value.
summary(person_fit$p.infitMSQ)
summary(person_fit$p.outfitMSQ)
```

- We can see that there is some variability in person fit, with infit MSE statistics ranging from 0.41 to 2.25, and outfit MSE statistics ranging from 0.17 to 7.30:

- From this fit analysis, we can see that the mean of the infit MSE and outfit MSE statistics are close to 1.0. 


```{r}
# Also, you can use the "personMisfit" function to find the person who misfit 
misfit_person <- PersonMisfit(person_pa) 
# This function counts the number of persons who do not fit the Rasch model. More specifically, it returns the proportion and frequency of persons - or more generally cases - who exceed a Chi-square based Z-value of 1.96 (suggesting a statistically significant deviation from the predicted response pattern).
misfit_person
# About 1.6043% persons are misfitting
misfit_person$count.misfit.Z
# The detailed number for misfitting is 6
misfit_person$total.persons
# This is the number of persons for whom a fit value was estimated
```

### Item/Person Map
The "eRm" package provides plotting function to show the location of item/person on both logit scale and their t stastistics.

```{r}
plotPWmap(Di_Rasch_model,imap=TRUE) # You can plot the Item Map
plotPWmap(Di_Rasch_model,pmap=TRUE) # You can even put the person and item inside one map
```
Also, we can construct a plot that shows item and person locations in the same graphical display (a Person-Item Map). 

```{r}
#To do this, use the following code:
plotPImap(Di_Rasch_model, sorted = TRUE)
```
In this plot, we should consider the degree to which there is evidence of overlap between item and person locations (targeting).

We can also examine the individual items’ ordering on the logit scale with reference to our theory about the expected ordering.


### Item Characteristic Curves (ICC)
The *IRFs/ICCs* that we have been looking at are based on model-expected response probabilities.

```{r}
plotICC(Di_Rasch_model,mplot=TRUE,ask = FALSE)
```
Note that the R package did not plot the observed probability.

### Plots of standardized residuals
Let’s use some graphical displays to examine item fit in more detail. Please review our lecture materials for details about these displays. These plots show the standardized residual for the difference between the observed and expected response for each person on the item of interest.
```{r}
# Collect the residual values from the Itemfit results
stresid <- Di_itemfit$st.res
# before constructing the plots, find the max & min residuals:
max.resid <- ceiling(max(stresid))
min.resid <- ceiling(min(stresid))
# The code below will produce standardized residual plots for each of the items in our example dataset in the “Plots” window on the bottom right of your R Studio window:
for(item.number in 1:ncol(stresid)){
  
  plot(stresid[, item.number], ylim = c(min.resid, max.resid),
       main = paste("Standardized Residuals for Item ", item.number, sep = ""),
       ylab = "Standardized Residual", xlab = "Person Index")
  abline(h = 0, col = "blue")
  abline(h=2, lty = 2, col = "red")
  abline(h=-2, lty = 2, col = "red")
  
  legend("topright", c("Std. Residual", "Observed = Expected", "+/- 2 SD"), pch = c(1, NA, NA), 
         lty = c(NA, 1, 2),
         col = c("black", "blue", "red"), cex = .8)
  
}

```

Then, we are going to plot the item characteristic curves (item response functions; IRFs) with the observed responses overlaid on them. These are sometimes called empirical IRFs.

```{r}
for(item.number in 1:ncol(stresid)){
  plotICC(Di_Rasch_model, item.subset = item.number, empICC = list("raw"), empCI = list())
}
```


## Supplmentary Learning Materials
1. [*What do Infit and Outfit, Mean-Square and Standardized mean?*](https://www.rasch.org/rmt/rmt162f.htm)

2. [Wright, B.D., & Masters, G.N. (1990). Computation of OUTFIT and INFIT Statistics. *Rasch Measurement Transactions,3(4)* p.84-85.](https://www.rasch.org/rmt/rmt34e.htm)

3. [Estimation methods: JMLE, PROX, WMLE, CMLE](https://www.winsteps.com/winman/estimation.htm)
