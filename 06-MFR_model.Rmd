# Many Facet Rasch Model

## Motivation

Some assessment situations include additional variables of interest beyond person responses and item difficulty.

> Aspects of the measurement process that “routinely and systematically interpose themselves between the ability of the candidates and the difficulty of the test” (B&F, p. 167)

Examples of “Interposing” Variables

- Raters in a constructed-response assessment

- Demographic variables (e.g., gender, race/ethnicity, best language)

- Item/prompt type

- Domain (analytic rubrics)

## Introduction to Many-Facet Rasch (MFR) Model

The Many-Facet Rasch Model allows the researcher to specify explanatory facets beyond person and item locations. The model is user-specified and can be applied to any ordinal scoring scheme (dichotomous, polytomous; rating scale or partial-credit). 

The Many-Facet Rasch Model was developed by Mike Linacre (1989/1992) in his dissertation research with Ben Wright at Chicago. Since that time, it has been widely used across many measurement contexts.

## Why use the MFR model?

One popular use of the MFR model is to examine and mitigate differences in rater (judge) severity in performance assessments, where raters score test-taker performances. In these assessments, we want to make sure that our conclusions about test-takers' achievement don’t depend on the “luck of the rater draw” (Engelhard’s often-used phrase).

The MFR model allows us to adjust test-taker achievement estimates for differences in rater severity (and/or other facets), as long as:

- There is evidence of acceptable model-data fit for all facets

- There is sufficient connectivity in the data (raters and test-takers are linked; more on that later in the semester)

Are we adding dimensions by considering other facets?

- Not if we believe that the facets are key aspects of the measurement process that systematically influence the overall score.

- The MFR model allows us to test hypotheses of invariant calibrations across levels of facets (e.g., individual raters, item types, demographic subgroups)

## MFR Calibrations

MFR model provides location estimates for individual elements within each facet such as individual students, items, raters, domains, subgroups, subsets. These can be plotted on the variable map as before.

There is no single equation for MFR model. You will decide which explanatory facets to include, as well as the scale structure/underlying model, including:

- Dichotomous Rasch model (Rasch, 1960)
- Polytomous models:
  - Rating Scale Model (Andrich, 1978)
  - Partial-Credit Model (Masters, 1982)

For the PC-MFR model, the researcher decides the facet across which the scale varies (raters, prompts, subgroups, etc.).

## R-Lab: Runing the Rasch Many-Facet Model in R

NOTE: This lab is based on the TAM package for R. Although it is possible to run MFR models using this package, its use for this model is not currently widespread among researchers. In addition, there is not a direct correspondence between the information that TAM provides and the information that Facets provides about the MFR model results. Please keep this in mind when using the package for your MFR model analyses and watch for updates in the Rasch literature.

### Prepare the Dataset

Let's type in the data into R first. Please see the Facets lab guide for details about the data.

```{r message=FALSE}
g.data <- matrix(c(1,1,5,5,3,5,3,
1,2,9,7,5,8,5,
1,3,3,3,3,7,1,
1,4,7,3,1,3,3,
1,5,9,7,7,8,5,
1,6,3,5,3,5,1,
1,7,7,7,5,5,5,
2,1,6,5,4,6,3,
2,2,8,7,5,7,2,
2,3,4,5,3,6,6,
2,4,5,6,4,5,5,
2,5,2,4,3,2,3,
2,6,4,4,6,4,2,
2,7,3,3,5,5,4,
3,1,5,5,5,7,3,
3,2,7,7,5,7,5,
3,3,3,5,5,5,5,
3,4,5,3,3,3,1,
3,5,9,7,7,7,7,
3,6,3,3,3,5,3,
3,7,7,7,7,5,7),ncol=7,byrow=TRUE)
```

This data set contains 7 columns. The first column is the ID indicator for Sr. Scientist, ranging from 1 to 3 to indicate the three Sr. Scientists. The second column is the ID indicator for Jr. Scientist, ranging from 1 to 7. The rest of the columns (3-7) are the responses matrix, ranged from 1-9. Each column represents a trait that is being measured.

Note about the data structure:
This dataset is organized in long format (rather than wide format). As a result, there are multiple rows for each Sr. Scientist. This is because each Sr. Scientist rated all of the Jr. Scientists.

Now, let's add column names to our dataset. First, we will turn the data into a data.frame object. Then, we will add column names.

```{r}
g.data <- as.data.frame(g.data)
colnames(g.data) <- c("subjects","raters","Trait_a","Trait_b","Trait_c","Trait_d","Trait_e")
```

### Run the MFR model using the TAM package

```{r message=FALSE}
library(TAM)
```

```{r message=FALSE, results='hide'}
g.facet <- g.data[,"raters",drop=FALSE] # specify which facets will be included in the model (Here, we are including raters as a facet. Items ("assessment opportunities"; occassions on which the object of measurement is observed) are included as a facet by default)
g.pid <- g.data$subjects # specify the ID for the object of measurement (Here, this is the Jr. Scientist)
g.resp <- g.data[,-c(1:2)] # Indicate the response matrix
g.formulaA <- ~ item + raters + step # Model formula for RS-MFR model (multiply (raters * step) to specify a PC-MFR model where the scale varies by rater)
g.model <- tam.mml.mfr(resp=g.resp,facets=g.facet,formulaA=g.formulaA,pid=g.pid)
# Run the many-facet model
```

```{r}
summary(g.model) # Check the model summaries
```

### Test-taker Estimates
```{r}
## Person (test-taker) Estimates
# Compute person fit statistics
person.fit <- tam.personfit(g.model)
person.fit # Check the person infit/outfit
# Person's Ability
persons.mod <- tam.wle(g.model)
theta <- persons.mod$theta
theta # Print out the person's ability
```

### Item's Estimates
```{r}
## Compute Item fit statistics
item.fit <- msq.itemfit(g.model)
summary(item.fit) # fit is shown for the rater*item combinations

library(knitr) # Use the knitr package to print out the result table
kable(g.model$xsi.facets,digits=2)
```

### Plots

#### The WrightMap 

```{r}
library(WrightMap)
IRT.WrightMap(g.model)
```

The tam.thresholds command provides us with the estimated difficulty for each item-by-rater 

```{r}
thr <- tam.threshold(g.model)
item.labs <- c("Trait_a", "Trait_b", "Trait_c", "Trait_d", "Trait_e")
rater.labs <- c("rater1", "rater2", "rater3")
```

Now we need to turn it into a matrix formatted the way WrightMap expects. We could organize it by item:

```{r}
thr1 <- matrix(thr, nrow = 5, byrow = TRUE)
wrightMap(theta, thr1, label.items = item.labs, thr.lab.text = rep(rater.labs, each = 5))
```

Or by rater:

```{r}
thr2 <- matrix(thr, nrow = 3)
wrightMap(theta, thr2, label.items = rater.labs, thr.lab.text = rep(item.labs,  each = 3), axis.items = "Raters")
```
#### Plot Item Response Curves

```{r}
# Plot Item Response curves
plot(g.model, type="items")
# Plot expected response curves
plot(g.model, type="expected")
```

## Reference

Robert J. Sternberg & Elena L. Grigorenko (2001) Guilford's Structure of Intellect Model and Model of Creativity: Contributions and Limitations, Creativity Research Journal, 13:3-4, 309-316, DOI: 10.1207/S15326934CRJ1334_08


## Example 2:

As a second example, let's analyze some data from a performance assessment in which 21 raters scored 372 students' essays on four domains: Style, Organization, Conventions, and Sentence Formation

```{r}
# Input the data from the .csv file:
georgia_writing <- read.csv("georgia_writing.csv")
```

Note that the data are in *long* format: This means that there are multiple rows for each element within the object of measurement (multiple rows for each student).

```{r}
# Get a summary of the data
summary(georgia_writing)

# count the unique student ids:
length(unique(georgia_writing$student))
```

Next, let's prepare the data for analysis with TAM by shifting the scale so that the lowest category equals zero:

```{r}
# Recode the scale to begin at 0
georgia_writing[, -c(1:2)] <- georgia_writing[, -c(1:2)] - 1
```

Now, we can run a MFR model with the following facets: Students (object of measurement), Raters, and Domains. 

To work with the TAM package, we will treat Domains like items.

We will run a Rating Scale formulation of the model first.

```{r}

# First, ensure that the data are ordered by the object of measurement:
georgia_writing <- georgia_writing[order(georgia_writing$student),]


## set up the MFR model analysis:

# identify the facets besides items:
writing.facets <- georgia_writing[, c("rater"),drop=FALSE] 

# identify the object of measurement:
writing.pid <- georgia_writing$student 

# identify the response matrix:
writing.resp <- georgia_writing[,-c(1:2)]

# specify the RS-MFR model:
RS.writing.formula <- ~ item + rater + step 

# Run the RS-MFR model:

writing.model <- tam.mml.mfr(resp=writing.resp,facets=writing.facets,
                             formulaA=RS.writing.formula, pid=writing.pid)

# Request a summary of the model results:
summary(writing.model)

# Save the facet estimates:
facet.estimates <- writing.model$xsi.facets # all facets together

domain.estimates <- subset(facet.estimates, facet.estimates$facet == "item")

rater.estimates <- subset(facet.estimates, facet.estimates$facet == "rater")

threshold.estimates <- subset(facet.estimates, facet.estimates$facet == "step")


```

### Student Estimates
```{r}
## Student Estimates

# Student fit:
person.fit <- tam.personfit(writing.model)
person.fit # Check the person infit/outfit

# Student achievement estimates:
student.ach <- tam.wle(writing.model)
theta <- student.ach$theta
summary(theta)
```

### Domain/Rater Fit:
```{r}
## Compute rater and domain fit statistics 
# (note that these are called "items" in the TAM code).
rater_domain.fit <- msq.itemfit(writing.model)
summary(rater_domain.fit) # fit is shown for the rater*item combinations
```

### Plots

#### The WrightMap 

```{r}
library(WrightMap)
IRT.WrightMap(writing.model)


